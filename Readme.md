# LLM-Benchmark 

This is a list tracking good LLM Benchmarks. Unfortunately not all can be run with API endpoints. If there's any you'd like to use with API endpoints, create an issue. 

| Library | ChatCompletions | Completions | Custom Proxy | Comments |
| --- | --- | --- | --- | --- |
| [EvalPlus](https://github.com/evalplus/evalplus) | ✅ | ✅ | ✅ | Evaluates code gen |
| LM Eval harness | | ✅ | |
| [MT-Bench w/ LLM Judge](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge#how-to-get-gpt-35gpt-4claudes-answer) | ✅ | | | Evaluates chat assistants. Asks turn-by-turn conversation questions and then uses another LLM to evaluate results |
| RAGAS | ✅ | ✅ | |
| HELM   | ✅ | | [Link](https://github.com/stanford-crfm/helm/blob/main/demo.py) |
| [FLASK](https://github.com/kaistAI/FLASK) | | | |
| [bigcode-project](https://github.com/bigcode-project/bigcode-evaluation-harness) | | | |
| HumanEval (https://github.com/openai/human-eval) | | ✅ | ✅ |
| BigBench (https://github.com/google/BIG-bench) | | | |
| Fiddler (https://github.com/fiddler-labs/fiddler-auditor) | ✅ | ✅ | |
| LLM Attacks (https://github.com/llm-attacks/llm-attacks) | | | |
| GPT Fathom (https://github.com/GPT-Fathom/GPT-Fathom) | ✅ | ✅ | |
