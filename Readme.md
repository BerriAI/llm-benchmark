# LLM-Benchmark 

This is a list tracking good LLM Benchmarks. Unfortunately not all can be run with API endpoints. If there's any you'd like to use with API endpoints, create an issue. 

| Library | ChatCompletions | Completions | Custom Proxy |
| --- | --- | --- | --- |
| [EvalPlus](https://github.com/evalplus/evalplus) | ✅ | ✅ | ✅ |
| LM Eval harness | | ✅ | |
| RAGAS | ✅ | ✅ | |
| HELM   | ✅ | | [Link](https://github.com/stanford-crfm/helm/blob/main/demo.py) |
| [FLASK](https://github.com/kaistAI/FLASK) | | | |
| [bigcode-project](https://github.com/bigcode-project/bigcode-evaluation-harness) | | | |
| HumanEval (https://github.com/openai/human-eval) | | ✅ | ✅ |
| BigBench (https://github.com/google/BIG-bench) | | | |
| Fiddler (https://github.com/fiddler-labs/fiddler-auditor) | ✅ | ✅ | |
| LLM Attacks (https://github.com/llm-attacks/llm-attacks) | | | |
| GPT Fathom (https://github.com/GPT-Fathom/GPT-Fathom) | ✅ | ✅ | |
